{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Map Area\n",
    "\n",
    "## Location: Beijing, China.\n",
    "\n",
    "Map Url: \n",
    "\n",
    "- [https://mapzen.com/data/metro-extracts/metro/beijing_china/](https://mapzen.com/data/metro-extracts/metro/beijing_china/)\n",
    "\n",
    "- [https://www.openstreetmap.org/relation/912940](https://www.openstreetmap.org/relation/912940)\n",
    "\n",
    "## File Size\n",
    "\n",
    "The data file is about 181 MB in size (uncompressed).\n",
    "\n",
    "## Reason to Choose this Area\n",
    "\n",
    "I choose Beijing as the map area because I had lived in Beijing for several years, so I'm familar with this city and would like to check the quality of the map data from OpenStreetMap.org.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "There are five steps of this analysis:\n",
    "\n",
    "1. Generate the Sample data;\n",
    "\n",
    "2. Check how many different kinds of tags in the data;\n",
    "\n",
    "3. Audit potential problems for each tag;\n",
    "\n",
    "4. Fix the problems;\n",
    "\n",
    "5. Prepare the data to be inserted to a SQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate the Sample data\n",
    "\n",
    "Execute the follow command in the Shell:\n",
    "\n",
    "```sh\n",
    "python data/gen_sample.py\n",
    "```\n",
    "\n",
    "This will output a sample data file in `data/` folder with name `sample.osm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Check how Many Different Kinds of Tags in the Data\n",
    "\n",
    "Although this is not the requirement of this project, I'm a little curious about how many different kinds of tags are there in the data.  And I used the original data file instead of the sample data to check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nd', 1018804),\n",
      " ('node', 853320),\n",
      " ('tag', 360734),\n",
      " ('way', 127592),\n",
      " ('member', 61361),\n",
      " ('relation', 5657),\n",
      " ('bounds', 1),\n",
      " ('osm', 1)]\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from pprint import pprint\n",
    "\n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for event, ele in ET.iterparse(filename, events = ('start', 'end')):\n",
    "        if event == 'end':\n",
    "            if ele.tag in tags:\n",
    "                tags[ele.tag] += 1\n",
    "            else:\n",
    "                tags[ele.tag] = 1\n",
    "    return sorted(tags.items(), key = lambda x: (-x[1], x[0]))\n",
    "\n",
    "tags = count_tags('data/beijing_china.osm')\n",
    "pprint(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results shows there are totally 8 different kinds of tags in the data, and `nd`, `node`, `tag`, `way` are the most common tags in the data. Because the data that we will extract are mainly in these four kinds of tags, so the audit and clean process are focused on these tags. For `tag` tag, it may be the tag of a `node` tag or a tag of a `way` tag, so we call them as `nodes_tags` and `ways_tags`, seperately. For `nd` tags, we call them `ways_nodes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Audit Potential Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to audit potential problems of this data, I choose to focus on two aspects of the data quality:\n",
    "\n",
    "- *Data Fields Types*;\n",
    "\n",
    "- *Data Fields Validity*.\n",
    "\n",
    "**The code for audit process is in the `audit` function within the `audit.py` file, which takes the data path as input and return the audit result as output. The output is a dict, and its structure is shown as below:**\n",
    "\n",
    "```python\n",
    "{'field_types': \n",
    "    {'node':\n",
    "        {'id': (type_1, type_2, ..., type_n),\n",
    "         'lat': (type_1, type_2, ..., type_n),\n",
    "          ......\n",
    "         'timestamp': (type_1, type_2, ..., type_n)},\n",
    "     'way':\n",
    "         {'id': (type_1, type_2, ..., type_n),\n",
    "          'user': (type_1, type_2, ..., type_n),\n",
    "           ......\n",
    "          'timestamp': (type_1, type_2, ..., type_n)},\n",
    "     'node_tags':\n",
    "         {'k': (type_1, type_2, ..., type_n),\n",
    "          'v': (type_1, type_2, ..., type_n)},\n",
    "     'way_tags':\n",
    "         {'k': (type_1, type_2, ..., type_n),\n",
    "          'v': (type_1, type_2, ..., type_n)},\n",
    "     'way_nodes':\n",
    "         {'ref': (type(int()))}\n",
    "     },\n",
    " 'field_validity':\n",
    "     {\n",
    "         'node': {'lat': ['min', 'max'], \n",
    "                  'lon': ['min', 'max'], \n",
    "                  'timestamp': ['min', 'max']},\n",
    "         'way': {'timestamp': ['min', 'max']},\n",
    "         'node_tags': {'postcode': ('wrong_value', ...)},\n",
    "         'way_tags': {'name_en': {'unknown_way_type': ['way_name',......]}, \n",
    "                      'postcode': ('wrong_value', ......)},\n",
    "         'way_nodes': {}\n",
    "     }\n",
    "}\n",
    "```\n",
    "\n",
    "Within the `audit(osm_file)` function:\n",
    "\n",
    "- `update_field_types(e, tag)`: this function will update the set of its field types based on current element for a given tag, so after iterative across the file, we can get the results of the `field_types`;\n",
    "\n",
    "- `validate_*`: the functions that start with `validate_` will check the validity of some predefined fields based on the type of the current element, so after iterative across the file, we can get the result of `field_validity`.\n",
    "\n",
    "In addition, there are several helper functions within the `audit()` function to avoid too much repeated code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems Encountered in the Map\n",
    "\n",
    "1. Over Abbreviated Street Names: e.g. (`\"W. Dengshikou Str\"`)\n",
    "\n",
    "2. Inconsistent Street Names: e.g. (`\"Yongfeng Lu\"`, `\"Liangshidian jie\"`)\n",
    "\n",
    "3. Incorrect Postal Codes: e.g. (`k=\"addr:postcode\" v=\"010-62332281\"`)\n",
    "\n",
    "> *Note: Because the map area that I choose is in China, which makes the field `addr:street` is in Chinese, however, this is an english project and there is a field `name:en`, so after checking the field `name:en` and `addr:street`, I found the number of field `name:en` is much more than the number of field `addr:street`, and I believed the field `name:en` of the element `way` gives the information about the name of the location. So I decided to use field `name:en` to audit instead of field `addr:street`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### About the `way` names\n",
    "\n",
    "After the audit process, I noticed that the most common problems are about names, and there are three kinds of the name problems:\n",
    "\n",
    "1. Over Abbreviated Street Names, such as:\n",
    "\n",
    "    - `\"W. Dengshikou Str\"`: `\"Str\"` should be `\"Street\"`;\n",
    "    \n",
    "    - `\"Lugu Rd.\"`: `\"Rd.\"` should be `Road`;\n",
    "    \n",
    "2. Inconsistent street names which use Chinese \"Pinyin\" to represent the English name, such as:\n",
    "\n",
    "    - `\"Yongfeng Lu\"`: `\"Lu\"` should be `Road`;\n",
    "    \n",
    "    - `\"Liangshidian jie\"`: `\"jie\"` should be `Street`;\n",
    "    \n",
    "3. Uncommon street names which are difficult to deal with, such as:\n",
    "\n",
    "    - `'Habor': set(['Solana Blue Habor'])`\n",
    "    \n",
    "    - `'Cheng)': set(['Interwest (Zhu Yu Cheng)'])`\n",
    "    \n",
    "    - `'Middle': set(['North 3rd Ring Road Middle'])`\n",
    "    \n",
    "    \n",
    "For the first and second kinds of problems, they can be easily cleaned up programmatically, however, for the third kind of problems, it is hard to cleaned up programmatically. The reason is these street names are not regularly, even the local people can't easily figure out their english name, so these street names have to be dealt with one by one with care.\n",
    "\n",
    "Therefore, in this case study, I will only focus on the first two kinds of problems and ignore the third."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the postal codes\n",
    "\n",
    "After running the audit script, I found a obvious error with the postcode in the sample data: `k=\"addr:postcode\" v=\"010-62332281\"`. \"010-62332281\" is a telephone number and not a postcode. Since it's hard to get the postcode from the telephone number automatically, the proper solution is just delete this wrong postcode (it maybe possible to use geocode api of Google Maps based on the street name to obtain the postcode, but this is beyond the scope of this case study and the result from geocode api still need to be checked, so I just choose the simple way to deal with this postcode problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fix the Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For the problems of way names, I use the follow function to fix it:\n",
    "\n",
    "```python\n",
    "def update_way_names(name, mapping):\n",
    "    for k, v in mapping.items():\n",
    "        if k in name:\n",
    "            name = name.replace(k, mapping[k])\n",
    "            return name\n",
    "    return name\n",
    "```\n",
    "\n",
    "and for the problems of the postcode, I just ignore the problem filed when converting osm file to csv file in the `data.py` script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare the data to be inserted to a SQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once `audit.py` and `data.py` are completed, the osm file will be converted to csv file after execute the `data.py` script, and some of the problems will be fixed in the meantime.\n",
    "\n",
    "After this converting process, it is very easy to import the csv files to a SQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview and Additional Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File sizes\n",
    "\n",
    "- beijing_china.osm: 181 MB;\n",
    "\n",
    "- osm_beijing.db: 167 MB;\n",
    "\n",
    "- nodes.csv: 67 MB;\n",
    "\n",
    "- ways.csv: 7.2 MB;\n",
    "\n",
    "- nodes_tags.csv: 2.9 MB;\n",
    "\n",
    "- ways_tags.csv: 8.2 MB;\n",
    "\n",
    "- ways_nodes.csv: 23 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT COUNT(*) FROM nodes;\n",
    "```\n",
    "\n",
    "The result is: 853320"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT COUNT(*) FROM ways;\n",
    "```\n",
    "\n",
    "The result is: 127592"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of unique users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT COUNT(DISTINCT(T.uid)) FROM \n",
    "(SELECT uid FROM nodes UNION ALL \n",
    "SELECT uid FROM ways) as T;\n",
    "```\n",
    "\n",
    "The result is: 1798"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 contributing users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT T.user, COUNT(*) AS num FROM \n",
    "(SELECT user FROM nodes UNION ALL SELECT user FROM ways) as T \n",
    "GROUP BY T.user \n",
    "ORDER BY num DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "The result is: \n",
    "\n",
    "- `Chen Jia|237945`\n",
    "\n",
    "- `R438|142732`\n",
    "\n",
    "- `hanchao|66853`\n",
    "\n",
    "- `ij_|51901`\n",
    "\n",
    "- `Алекс Мок|47522`\n",
    "\n",
    "- `katpatuka|23521`\n",
    "\n",
    "- `m17design|21599`\n",
    "\n",
    "- `Esperanza36|18527`\n",
    "\n",
    "- `nuklearerWintersturm|16474`\n",
    "\n",
    "- `RationalTangle|13748`\n",
    "\n",
    "It seems that the contributions of users is skewed, we can use the followed SQL to compute the overall contributions of the top 10 contributing users:\n",
    "\n",
    "```sql\n",
    "SELECT SUM(NUM.num) FROM \n",
    "(SELECT T.user, COUNT(*) AS num FROM \n",
    "    (SELECT user FROM nodes UNION ALL SELECT user FROM ways) as T \n",
    "    GROUP BY T.user ORDER BY num DESC LIMIT 10) as NUM;\n",
    "```\n",
    "\n",
    "The result shows that the overall contributions of the top 10 contributing users is 640822, which makes up about 65.3% of all users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Amenities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT value, COUNT(*) as num FROM \n",
    "nodes_tags WHERE key='amenity' \n",
    "GROUP BY value \n",
    "ORDER BY num DESC \n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "The result is: \n",
    "\n",
    "- `restaurant|1267`\n",
    "\n",
    "- `bank|452`\n",
    "\n",
    "- `toilets|359`\n",
    "\n",
    "- `fast_food|328`\n",
    "\n",
    "- `cafe|274`\n",
    "\n",
    "- `school|161`\n",
    "\n",
    "- `telephone|151`\n",
    "\n",
    "- `bar|142`\n",
    "\n",
    "- `parking|135`\n",
    "\n",
    "- `atm|114`\n",
    "\n",
    "Well, it's interesting to find that the most popular amenity is restaurant, however, this is not a surprise because this is Beijing.😀 Further, I would like to check the top 10 popular cuisines in Beijing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Popular Cuisines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT nodes_tags.value, COUNT(*) as num \n",
    "FROM nodes_tags JOIN \n",
    "(SELECT DISTINCT(id) FROM nodes_tags WHERE value='restaurant') as T \n",
    "ON nodes_tags.id=T.id \n",
    "WHERE nodes_tags.key='cuisine' \n",
    "GROUP BY nodes_tags.value \n",
    "ORDER BY num DESC \n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "The result is: \n",
    "\n",
    "- `chinese|166`\n",
    "\n",
    "- `japanese|21`\n",
    "\n",
    "- `italian|17`\n",
    "\n",
    "- `pizza;american|15`\n",
    "\n",
    "- `regional|11`\n",
    "\n",
    "- `international|10`\n",
    "\n",
    "- `pizza|9`\n",
    "\n",
    "- `american|7`\n",
    "\n",
    "- `asian|7`\n",
    "\n",
    "- `german|5`\n",
    "\n",
    "It's very clear that the Chinese restaurant is the most popular one, and the number of Chinese restaurant is far more than the other cuisines. \n",
    "\n",
    "From the above results, we can also notice that there are repeated cuisines in the data: `pizza;american` vs. `pizza` and `american`, which need to be cleaned up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Ideas\n",
    "\n",
    "### OSM Data need a New Data Type: `area`\n",
    "\n",
    "When I audit the data I found that `ways` not only contains \"ways\" but also contains \"areas\": such as building, university, park, etc. In my opinion, we need three kinds of abstract spatial data types to represent the spatial data: point, line, and area. In the OSM map data, there are only \"node\" and \"way\" which represents the \"point\" and \"line\", seperately. So there is a missing data type: area. The reason to seperate the \"way\" and \"area\" is that \"way\" is more suitable to stand for the \"road\", \"street\" and so on, and \"area\" is more suitable to stand for the \"building\", \"park\" and so on. Also, \"line\" is made up of \"point\", and \"area\" is made up of \"line\", so if we add the third type: \"area\", then the data structure will be more complete and less confused.\n",
    "\n",
    "### Potential problems/challenges\n",
    "\n",
    "Besides the benefits of the proposed improvement, there exist some potential problems/challenges to be addressed. The most obvious one is that sometimes it is difficult to decide whether a place should be an \"area\" or just a \"way\". For example, in many cities of China, there will be some commercial pedestrian streets (with restaurants and cloth shops, etc.) for shopping, basically we can treat them as \"way\". However, some commercial pedestrian streets are more like a square than a street, which indicates that it should be treated as \"area\". Therefore, the user who would like to create the data should consider this problem prudently and then choose a suitable type, which may need some professional knowledge.\n",
    "\n",
    "The reason for this challenges is that \"area\" is also made up of multiple points of which the start point is the same as the end point, so \"area\" is essentially a special type of \"line\". In addition, I think this maybe the reason that OSM map data only contains points and lines. \n",
    "\n",
    "However, this should not be the reason for the missing of the \"area\" type, because \"area\" and \"line\" are very different spatial data abstract types and they should have different set of properties. So I would suggest that \"area\" should be added to the abstract data types of the OSM data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From this projct I learned that data cleaning is a time-consuming process, it is not as cool as the machine learning models, but it's a really important part of the whole Data Science project. Because if the quality of the input data is not good, then the model can not be a good model, because the model is learned from the data. However, sometimes maybe there is no end of the data cleaning process, because if the data is big enough we can not guarantee that there is no error in the data. We can only audit and clean the data thoroughly to make the data better and better until the data quality meet the requirement of the current project.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
